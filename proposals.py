import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow_probability import distributions as tfd


class Bootstrap:

    def __init__(self, model):
        """
        Proposal according to the latent process dynamics.

        :param model: model object, see models.py
        """
        self.model = model
        self.bootstrap = True  # check whether the proposal object is Bootstrap,
        # used in the log-weights computation in the particle filter

    def propagate(self, x, n):
        """
        Computes the new particles propagated according to prior distribution

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :param n: iteration number, used when observations need to be taken into account
        :return: tf.Tensor of particles, first dimension is the number of particles
        """
        return self.model.propagate_latent_prior(x, n)

    def list_trainable(self):
        """
        Provides the list of proposal-specific parameters that need to be trained, not used in this specific class.
        :return: empty list
        """
        return []

    def variables_to_watch(self):
        """
        Provides a list of values of proposal-specific parameters that we want to monitor,
        not used in this specific class.
        :return: empty list
        """
        return []



class DeepGaussian:

    def __init__(self, model, nodes_mu, nodes_sigma):

        """
        Gaussian proposal with the neural networks (NNs) that provide mean (vector if model dim > 1) and standard deviation (vector)
        of the distribution, from which new particles are sampled.

        :param model: model object, see models.py
        """

        self.model = model
        self.dim_input = self.model.dim + self.model.dim_obs  # dimension of the input of the NN
        self.dim_output = self.model.dim  # dimension of the output of the NN
        self.bootstrap = False

        # NN for the mean (vector)
        tf.random.set_seed(0)
        self.nn_mu = tf.keras.Sequential()
        self.nn_mu.add(layers.Dense(nodes_mu, activation='relu', dtype=tf.float64))
        self.nn_mu.add(layers.Dense(self.dim_output, activation='linear', dtype=tf.float64))
        self.nn_mu.build((None, self.dim_input))

        # NN for the standard deviation (vector)
        tf.random.set_seed(0)
        self.nn_sigmas_diag = tf.keras.Sequential()
        self.nn_sigmas_diag.add(layers.Dense(nodes_sigma, activation='relu', dtype=tf.float64))
        self.nn_sigmas_diag.add(layers.Dense(self.dim_output, activation='softplus', dtype=tf.float64))
        self.nn_sigmas_diag.build((None, self.dim_input))

    def propagate(self, x, n):
        """
        Computes the new particles propagated according to a (multivariate) Gaussian distribution,
        with mean (vector) and standard deviation (vector) generated by NNs

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :param n: iteration number, used when observations need to be taken into account
        :return: tf.Tensor of particles, first dimension is the number of particles
        """
        self.compute_parameters(x, n)  # compute proposal parameters
        # sampling from Gaussian with reparameterization trick
        return self.mu + tf.math.multiply(self.sigmas_diag,
                                          tf.random.normal(list(x.shape[:-1])+[self.dim_output], dtype=tf.float64))
    
    def compute_parameters(self, x, n):
        """
        Compute the proposal parameters according to the NNs.
        Each current particle value is input together with the next observation.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :param n: iteration number, used when observations need to be taken into account
        """
        
        
        # For all other models 
        y = tf.broadcast_to(self.model.y[..., n], list(x.shape[:-1])+[self.model.dim_obs])
        
        inputs = tf.concat([x, y], axis=-1)
        
        
        self.mu = self.nn_mu(inputs)
        self.sigmas_diag = self.nn_sigmas_diag(inputs)

    def log_proposal_density(self, x_old, x_new, n):
        """
        Computes the log-density of the proposal for the new particles, with the means and standard deviations
        previously obtained through the NNs.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :return: tf.Tensor of log-densities, shape [N]
        """

        self.compute_parameters(x_old, n)
        if self.dim_output == 1:
            return tf.squeeze(tfd.Normal(self.mu, self.sigmas_diag).log_prob(x_new), axis=-1)
            # return tfd.Normal(tf.stop_gradient(self.mu), tf.stop_gradient(self.sigmas_diag)).log_prob(x)
        else:
            return tfd.MultivariateNormalDiag(self.mu, self.sigmas_diag).log_prob(x_new)
    
    
    def prior_mean(self, x, k,a0,a1,a2):
        return a0 * x + a1 * x / (1 + x**2) \
                + a2* tf.math.cos(1.2 * (k - 1))
        
    def log_proposal_density_growth(self, x_old, y,x_new,a0,a1,a2,k):
        """
        Computes the log-density of the proposal for the new particles, with the means and standard deviations
        previously obtained through the NNs.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :return: tf.Tensor of log-densities, shape [N]
        """

        x_tilde = self.prior_mean(x_old,k,a0,a1,a2)

        x_tilde = tf.cast(x_tilde, dtype=tf.float64)
        x_tilde = tf.expand_dims(x_tilde, axis=-1)
        y = tf.cast(y, dtype=tf.float64)
        y = tf.expand_dims(y, axis=-1)
        # Now you can concatenate them safely
        inputs = tf.concat([x_tilde, y], axis=-1)

        inputs = tf.reshape(inputs, (1, 2)) 
        self.mu = self.nn_mu(inputs)
        self.sigmas_diag = self.nn_sigmas_diag(inputs)
        
        dist = tfd.Normal(self.mu, self.sigmas_diag)
        log_prob_elements = tf.squeeze(dist.log_prob(x_new)).numpy()
        return log_prob_elements

        
    def list_trainable(self):
        """
        Provides the list of proposal-specific parameters that need to be trained.
        :return: list of all neural networks' weights
        """
        return self.nn_mu.trainable_weights + self.nn_sigmas_diag.trainable_weights

    def variables_to_watch(self):
        """
        Provides a list of values of proposal-specific parameters that we want to monitor.
        :return: empty list, customizable
        """
        return []
    
    def weights_nn(self):
        return self.nn_mu.trainable_weights + self.nn_sigmas_diag.trainable_weights
    
    
class DeepGaussian_growth:

    def __init__(self, model, nodes_mu, nodes_sigma):

        """
        Gaussian proposal with the neural networks (NNs) that provide mean (vector if model dim > 1) and standard deviation (vector)
        of the distribution, from which new particles are sampled.

        :param model: model object, see models.py
        """

        self.model = model
        self.dim_input = self.model.dim + self.model.dim_obs  # dimension of the input of the NN
        self.dim_output = self.model.dim  # dimension of the output of the NN
        self.bootstrap = False

        # NN for the mean (vector)
        tf.random.set_seed(0)
        self.nn_mu = tf.keras.Sequential()
        self.nn_mu.add(layers.Dense(nodes_mu, activation='relu', dtype=tf.float64))
        self.nn_mu.add(layers.Dense(self.dim_output, activation='linear', dtype=tf.float64))
        self.nn_mu.build((None, self.dim_input))

        # NN for the standard deviation (vector)
        tf.random.set_seed(0)
        self.nn_sigmas_diag = tf.keras.Sequential()
        self.nn_sigmas_diag.add(layers.Dense(nodes_sigma, activation='relu', dtype=tf.float64))
        self.nn_sigmas_diag.add(layers.Dense(self.dim_output, activation='softplus', dtype=tf.float64))
        self.nn_sigmas_diag.build((None, self.dim_input))

    def propagate(self, x, n):
        """
        Computes the new particles propagated according to a (multivariate) Gaussian distribution,
        with mean (vector) and standard deviation (vector) generated by NNs

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :param n: iteration number, used when observations need to be taken into account
        :return: tf.Tensor of particles, first dimension is the number of particles
        """
        self.compute_parameters(x, n)  # compute proposal parameters
        # sampling from Gaussian with reparameterization trick
        return self.mu + tf.math.multiply(self.sigmas_diag,
                                          tf.random.normal(list(x.shape[:-1])+[self.dim_output], dtype=tf.float64))
    
    def compute_parameters(self, x, n):
        """
        Compute the proposal parameters according to the NNs.
        Each current particle value is input together with the next observation.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :param n: iteration number, used when observations need to be taken into account
        """

        # For all other models 
        y = tf.broadcast_to(self.model.y[..., n], list(x.shape[:-1])+[self.model.dim_obs])
        
        # Istead of the state take the prior mean as an input
        x_tilde = self.model.prior_mean(x,n)
        inputs = tf.concat([x_tilde, y], axis=-1)
        
        self.mu = self.nn_mu(inputs)
        self.sigmas_diag = self.nn_sigmas_diag(inputs)

    def log_proposal_density(self, x_old, x_new, n):
        """
        Computes the log-density of the proposal for the new particles, with the means and standard deviations
        previously obtained through the NNs.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :return: tf.Tensor of log-densities, shape [N]
        """

        self.compute_parameters(x_old, n)
        if self.dim_output == 1:
            return tf.squeeze(tfd.Normal(self.mu, self.sigmas_diag).log_prob(x_new), axis=-1)
            # return tfd.Normal(tf.stop_gradient(self.mu), tf.stop_gradient(self.sigmas_diag)).log_prob(x)
        else:
            return tfd.MultivariateNormalDiag(self.mu, self.sigmas_diag).log_prob(x_new)
    
    def prior_mean(self, x, k,a0,a1,a2):
        return a0 * x + a1 * x / (1 + x**2) \
                + a2* tf.math.cos(1.2 * (k - 1))
        
    def log_proposal_density_growth(self, x_old, y,x_new,a0,a1,a2,k):
        """
        Computes the log-density of the proposal for the new particles, with the means and standard deviations
        previously obtained through the NNs.

        :param x: tf.Tensor of particles, first dimension is the number of particles
        :return: tf.Tensor of log-densities, shape [N]
        """

        x_tilde = self.prior_mean(x_old,k,a0,a1,a2)

        x_tilde = tf.cast(x_tilde, dtype=tf.float64)
        x_tilde = tf.expand_dims(x_tilde, axis=-1)
        y = tf.cast(y, dtype=tf.float64)
        y = tf.expand_dims(y, axis=-1)
        #concatenate them safely
        inputs = tf.concat([x_tilde, y], axis=-1)

        inputs = tf.reshape(inputs, (1, 2)) 
        self.mu = self.nn_mu(inputs)
        self.sigmas_diag = self.nn_sigmas_diag(inputs)
        
        dist = tfd.Normal(self.mu, self.sigmas_diag)
        log_prob_elements = tf.squeeze(dist.log_prob(x_new)).numpy()
        return log_prob_elements

        
    def list_trainable(self):
        """
        Provides the list of proposal-specific parameters that need to be trained.
        :return: list of all neural networks' weights
        """
        return self.nn_mu.trainable_weights + self.nn_sigmas_diag.trainable_weights

    def variables_to_watch(self):
        """
        Provides a list of values of proposal-specific parameters that we want to monitor.
        :return: empty list, customizable
        """
        return []
    
    def weights_nn(self):
        return self.nn_mu.trainable_weights + self.nn_sigmas_diag.trainable_weights
    














